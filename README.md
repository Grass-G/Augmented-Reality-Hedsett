# Augmented-Reality-Hedsett
Note: This is not the final version as some features that i added arent working as expected and will fix them later, here "headset" is reffering to a google cardboard Hedset running android. THIS PROJECT IS CURRENT UNFINISHED. PARTS OF THE CODE IS AI GENERATED.
Prerequsites: You must have deepseek-r1:7b installed via ollama and have stleast a 8GB graphics card stpporting CUDA, 
              Have a hedset's camera streaming locally as a IP camera (http://192.168.1.8:4747/) or modify the code according the the IP assigned.
              Have a webpage open on the headset (http://192.168.1.3:5000) to recive the stream
              Have CUDA 11.8 installed.
              Have Python 3.11+ installed.
              Install the ibraries mentioned in the code.

To run: open ollama and keep it runnign in background , modify the ip camera link according to your hedsets camera , double click the python file and open the webpage printed out on the headset and enter full screen mode for the best experience.

How it works:
-The camera from headset is streamed over to my desktop(Using Droidcam),
-Python intercepts the camera stream and runs it thru a pretrained model (YOLO 3m parameters).
-The code detects objects and creates a bounding box around it (for debugging purposes).
-There is a crosshair at the center for sleecting spefic objects.
-In the upper right corner an bottom right corner of the right video feed a HUD is presnet displaying the stats and the model output.
-There are two streams in landscape mode as the hedset has lenses presnet in it that takes the two streams and feeds one of each into each eye and our brain combines them into one stream ie- 2 eyes = 2 output feeds.
-flask is used to inatialise the above mentioned webpage.
-The code is optimised to run on a GPU backend as running it on a cpu causes over 1200ms of frame time , howe ever using the CUDA Kernal generated by AI i maanaged it to run on a gpu reducing the frame time to less than 30ms on average.
-It's also optimised by redusing the resolution and with the headset the stream actually feels realtime like almost zero delay between turning your hed and the stream updating.

Demo:  
![image](https://github.com/user-attachments/assets/ac56e92c-f4e4-4a48-95db-3336a6fafa7e)
![image](https://github.com/user-attachments/assets/c8048f65-3b66-4e90-a547-e761b123a846)
Here is the demo video : https://drive.google.com/file/d/1_GzHdImjZcgdDLb4DM1guD52HgbHRWAi/view?usp=sharing
Note that it says no object detected as nothing is on the crosshair at the moment. The second ss is what is ebing displayed on the hedset


Features:
-Has a integrated 8Billion Parameter LLM providing descriptions of objects its looking at (on corsshair)
-Uses computer vison via cv2 python lib and usses any ip camera to get its video stream
-Uses a pretrained model to detect objects(i plan on using a heavier model later this project is still in it early stages (I have been wanting to build someting like this project for a while now and the internshit gave me a reason to work on it)
-Time , fps , latency present in a HUD 
-Providing realtime descriptions of objects via a prompt that is forwarded to the LLM ( "Describe '{object_name}' in 25 words. Be technical and concise." )

Upcoming features:
-Zoom in and Zoom Out functions without worsening the quality
     -As the stream is alr manually limited to 480x640 zooming from in would worsen the quaity.
     -However i have a 1920x1080p camera on the headset , and the ip cam software has zoom in and zoom out functions as buttons on the cam's webpage http://192.168.1.8:4747/ .
     -But i would need to learn more about HTTP get requests to trigger those buttons via python.
     -And i would need a means of input (solved by the next feature).
-Hand detection(using mediapipe)
     -I plan on using media pipe to draw landmarks on my hands and fingers.
     -Use those landmarks to dtect different movement and poses.
     -and use those poses and inputs.
-Alexa home devices integration
     -leverage the Alexa Skills Kit (ASK) SDK for Python to build custom skills.
     -Use the above hand detection to provide commands for triggering actions ie turning on / off smart lights of the room im in , turn on / off or set speed of smart fans etc.....
     -use teachable matchine to detect the room im in.
-Voice commands
     -i plan on using speechrecogination and pyaudio to grab the input form the flask webpage and turn them into prompts for the LLM.
     -also if it starts with a cartain keyword for example "please" it uses it as a command for alexa.
-Switch video feed to diff ip camera ie my drones camera or a wifi security camera providing a different porespective.
and more......... However i would like to take my time learning more skills and finishing the project.

The cureent project ustlises Computer Vision , LLMs , Prompt engeneering etc and i feel its good enough to submit for now......


              
   
